{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a204e8c2-416b-439f-9ec1-35ff787599fb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Notebook Documentation\n",
    "\n",
    "## Goal\n",
    "The purpose of this notebook is to facilitate the upload and processing of background data from various survey providers, including Cint Access, Syno Distribution, Lucid, and PureSpectrum. Additionally, the notebook will handle the import of .sav files containing exported raw data, including non-completes, for analysis.\n",
    "\n",
    "## Process Overview\n",
    "1. **Data Upload**: Users will upload background data from the specified providers. This data typically includes respondent IDs, survey completion status, and other metadata.\n",
    "2. **Data Processing**: The notebook will process the uploaded data to prepare it for analysis. This may involve data cleaning, transformation, and merging of datasets from different sources.\n",
    "3. **Analysis of .sav Files**: The notebook will import and analyze .sav files, which are data files from SPSS (Statistical Package for the Social Sciences). These files contain survey responses, including those from respondents who did not complete the survey.\n",
    "4. **Reconciliation**: The final step is to generate an Excel file containing IDs that will be used to reconcile responses with the data collected by the Syno Survey tool.\n",
    "\n",
    "## Expected Outputs\n",
    "- An Excel file with reconciled IDs for matching survey responses with the collected data.\n",
    "\n",
    "## User Instructions\n",
    "- Users should upload all background data files from the specified providers to the notebook.\n",
    "- Users should also provide all .sav files from the exported raw data for analysis.\n",
    "- The notebook will process the data and generate the required Excel file for reconciliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3610fa39-0337-4e2d-9cb5-800ff26f6d3d",
     "showTitle": true,
     "title": "Import the libraries"
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 21,
    "jupyter": {
     "outputs_hidden": false
    },
    "lastExecutedAt": 1708746645431,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pyreadstat\nfrom datetime import datetime\nimport pandas as pd \nimport numpy as np \nimport os"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pyreadstat\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Send emails with Google\n",
    "from email.message import EmailMessage\n",
    "import smtplib\n",
    "import ssl\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "from utils.utils import read_data\n",
    "from utils.utils import read_distribution\n",
    "from utils.utils import read_purespectrum\n",
    "from utils.utils import read_lucid\n",
    "from utils.utils import lucidRemoves\n",
    "from utils.utils import lucidAdds\n",
    "from utils.utils import pureRemoves\n",
    "from utils.utils import pureAdds\n",
    "from utils.utils import distributionRemoves\n",
    "from utils.utils import distributionAdds\n",
    "from utils.utils import format_sheet\n",
    "\n",
    "# Uncomment if using an .env file to store secrets\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce04b47-aae1-456b-ae22-b5f5e7bf4fa2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. Read the data in SPSS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5381a6-88f5-4b7f-ae78-c3a3e0855937",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 18004,
    "lastExecutedAt": 1708746794667,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "data = read_data()",
    "outputsMetadata": {
     "0": {
      "height": 197,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: data/\n",
      "Reading file: data/Raw Data 797946 2024-03-13.sav\n"
     ]
    }
   ],
   "source": [
    "data = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a68589bd-d1f2-46e8-9702-e1b04fafc872",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Read the distribution files and merge them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6542162-6dd2-4c89-bd38-df0fc0252875",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 27,
    "lastExecutedAt": 1708746805419,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "distribution = read_distribution()",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: distribution/\n",
      "Reading file: distribution/4744_7201 Capio brand tracking - Sweden 2024 January_2024-03-13.csv\n",
      "Reading file: distribution/4744_7201 Capio brand tracking - Sweden 2024 March_2024-03-13.csv\n",
      "Reading file: distribution/4744_7201 Capio brand tracking - Sweden 2024 February_2024-03-13.csv\n"
     ]
    }
   ],
   "source": [
    "distribution = read_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f64fe5-32a7-4a66-b03e-a80f5a0bcea6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Read the purespectrum files and merge them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51947ce6-d64b-4bcc-9cf5-621b8e1eaf98",
     "showTitle": false,
     "title": ""
    },
    "executionCancelledAt": null,
    "executionTime": 106,
    "lastExecutedAt": 1708746814003,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "purespectrum = read_purespectrum()",
    "outputsMetadata": {
     "0": {
      "height": 57,
      "type": "stream"
     },
     "1": {
      "height": 97,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: purespectrum/\n"
     ]
    }
   ],
   "source": [
    "purespectrum = read_purespectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ca2378-881d-48a3-8764-3e1858f67af9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from directory: lucid/\n"
     ]
    }
   ],
   "source": [
    "lucid = read_lucid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 4744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bc135c2-1481-4585-9f10-8a35c39766b7",
     "showTitle": false,
     "title": ""
    },
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 719,
    "jupyter": {
     "outputs_hidden": false
    },
    "lastExecutedAt": 1708747916126,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "from datetime import datetime\nimport pandas as pd  # Ensure pandas is imported\n\n# Get the current date in the format MM_DD_YYYY\ncurrent_date = datetime.now().strftime(\"%m_%d_%Y\")\n\n# Use the current date in the filename\nfilename = f\"Reconciliation - {current_date}.xlsx\"\n\nwith pd.ExcelWriter(filename, engine='xlsxwriter') as writer:  # Specify the engine\n    print(\"Exporting the data to an Excel file\")\n    \n    # PureSpectrum reconciliations\n    print(\"Generating PureSpectrum Additions\")\n    pure_additions = pureAdds(purespectrum, data)\n    pure_additions.to_excel(writer, sheet_name=\"PureSpectrum - Add\", index=False)\n    format_sheet(writer, \"PureSpectrum - Add\", pure_additions)\n    \n    print(\"Generating PureSpectrum Removes\")\n    pure_removes = pureRemoves(purespectrum, data)  # Corrected typo in 'purespectrrum'\n    pure_removes.to_excel(writer, sheet_name=\"PureSpectrum - Remove\", index=False)\n    format_sheet(writer, \"PureSpectrum - Remove\", pure_removes)\n    \n    # Distribution reconciliations\n    print(\"Generating Distribution Additions\")\n    distribution_additions = distributionAdds(distribution, data)\n    distribution_additions.to_excel(writer, sheet_name=\"Distribution - Add\", index=False)\n    format_sheet(writer, \"Distribution - Add\", distribution_additions)\n    \n    print(\"Generating Distribution Removes\")\n    distribution_removes = distributionRemoves(distribution, data)\n    distribution_removes.to_excel(writer, sheet_name=\"Distribution - Remove\", index=False)\n    format_sheet(writer, \"Distribution - Remove\", distribution_removes)\n    \n    print(f\"File exported as {filename}\")",
    "outputsMetadata": {
     "0": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting the data to an Excel file\n",
      "Generating Distribution Additions\n",
      "This operation will affect 48 cells, please be patient\n",
      "Generating Distribution Removes\n",
      "This operation will affect 8 cells, please be patient\n",
      "File exported as Reconciliation - 03_13_2024.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Get the current date in the format MM_DD_YYYY\n",
    "current_date = datetime.now().strftime(\"%m_%d_%Y\")\n",
    "\n",
    "# Use the current date in the filename\n",
    "filename = f\"Reconciliation - {current_date}.xlsx\"\n",
    "\n",
    "with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:  # Specify the engine\n",
    "    print(\"Exporting the data to an Excel file\")\n",
    "\n",
    "    # Distribution reconciliations\n",
    "    if len(distribution) > 0:\n",
    "        print(\"Generating Distribution Additions\")\n",
    "        distribution_additions = distributionAdds(distribution, data)\n",
    "        distribution_additions.to_excel(writer, sheet_name=\"Distribution - Add\", index=False)\n",
    "        format_sheet(writer, \"Distribution - Add\", distribution_additions)\n",
    "        \n",
    "        print(\"Generating Distribution Removes\")\n",
    "        distribution_removes = distributionRemoves(distribution, data)\n",
    "        distribution_removes.to_excel(writer, sheet_name=\"Distribution - Remove\", index=False)\n",
    "        format_sheet(writer, \"Distribution - Remove\", distribution_removes)\n",
    "\n",
    "    # PureSpectrum reconciliations\n",
    "    if len(purespectrum) > 0:\n",
    "        print(\"Generating PureSpectrum Additions\")\n",
    "        pure_additions = pureAdds(purespectrum, data)\n",
    "        pure_additions.to_excel(writer, sheet_name=\"PureSpectrum - Add\", index=False)\n",
    "        format_sheet(writer, \"PureSpectrum - Add\", pure_additions)\n",
    "        \n",
    "        print(\"Generating PureSpectrum Removes\")\n",
    "        pure_removes = pureRemoves(purespectrum, data)\n",
    "        pure_removes.to_excel(writer, sheet_name=\"PureSpectrum - Remove\", index=False)\n",
    "        format_sheet(writer, \"PureSpectrum - Remove\", pure_removes)\n",
    "\n",
    "    # Lucidd reconciliations\n",
    "    if len(lucid) > 0:\n",
    "        print(\"Generating Lucid Additions\")\n",
    "        lucid_additions = lucidAdds(lucid, data)\n",
    "        lucid_additions.to_excel(writer, sheet_name=\"Lucid - Add\", index=False)\n",
    "        format_sheet(writer, \"Lucid - Add\", lucid_additions)\n",
    "        \n",
    "        print(\"Generating Lucid Removes\")\n",
    "        lucid_removes = lucidRemoves(lucid, data)\n",
    "        lucid_removes.to_excel(writer, sheet_name=\"Lucid - Remove\", index=False)\n",
    "        format_sheet(writer, \"Lucid - Remove\", lucid_removes)\n",
    "    \n",
    "    print(f\"File exported as {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8816ea-32c7-4b2b-bbfc-8a0d7cca2a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "email_sender = os.environ[\"EMAIL_SENDER\"]\n",
    "email_password = os.environ[\"EMAIL_PASSWORD\"]\n",
    "email_receiver = \"val@synoint.com\"\n",
    "\n",
    "em = MIMEMultipart()\n",
    "em[\"From\"] = email_sender\n",
    "em[\"To\"] = email_receiver\n",
    "# em[\"cc\"] = \"gna@synoint.com\"\n",
    "em[\"Subject\"] = f\"Reconciliations for P{project_id} file generated - {current_date}\"\n",
    "\n",
    "body = \"Please see the attached an Excel file with the requested reconciliations for Providers\"\n",
    "\n",
    "em.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "attachment = open(filename, \"rb\")\n",
    "\n",
    "p = MIMEBase(\"application\", \"octet-stream\")\n",
    "p.set_payload(attachment.read())\n",
    "encoders.encode_base64(p)\n",
    "p.add_header(\"Content-Disposition\", f\"attachment; filename={filename}\")\n",
    "em.attach(p)\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "\n",
    "with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context = context) as smtp:\n",
    "    smtp.login(email_sender, email_password)\n",
    "    smtp.sendmail(email_sender, email_receiver, em.as_string())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Reconciliate - Surveys Databricks",
   "widgets": {}
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
